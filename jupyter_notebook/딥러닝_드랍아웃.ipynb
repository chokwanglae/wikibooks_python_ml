{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드랍아웃 (DropOut)\n",
    "이번 실습을 통해서, 드랍아웃을 텐서플로우로 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층퍼셉트론 구조\n",
    "텐서플로우로 아래의 다층퍼셉트론을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터는 총 **60000** 개의 샘플이 있습니다.  \n",
    "테스트 데이터는 총 **10000** 개의 샘플이 있습니다.     \n",
    "모든 데이터는 **28 * 28** 의 픽셀을 가지고 있습니다.  \n",
    "\n",
    "MNIST 데이터는 아래의 그림처럼, 28*28의 픽셀 데이터입니다.  \n",
    "각각의 픽셀은 흑백 사진과 같이 0부터 255까지의 그레이 스케일을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0 from MNIST](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터에서 검증 데이터 분리하기\n",
    "학습 중간마다 검증 데이터로 모델의 성능을 측정하면 다음과 같은 장점이 있습니다.  \n",
    "1) 모델 학습이 제대로 진행되는 지 검증 정확도로 확인할 수 있습니다.  \n",
    "2) 학습 정확도는 올라가는 데 검증 정확도가 안올라가거나 떨어질 시, 조기 종료를 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data has 10000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0** 부터 **255** 까지의 그레이 스케일을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0** 부터 **9**까지의 이미지에 해당하는 숫자를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터는 **10000** 개의 샘플을 가지고 있습니다.  \n",
    "모든 테스트 데이터는 **28 * 28** 의 이미지입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 구조 변경하기\n",
    "다층퍼셉트론의 입력 레이어에 데이터를 넣기 위해서 2d tensor (28, 28)인 데이터를,  \n",
    "1d tensor (28*28, 1)의  형태로 바꿔줍니다.  \n",
    "이 말은 행렬 형태의 데이터를 배열 형태의 데이터로 변경한다는 의미와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(50000, 784)\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정규화\n",
    "데이터 정규화는 보통 학습 시간을 단축하고, 더 나은 성능을 구하도록 도와줍니다.  \n",
    "MNIST 데이터의 모든 값은 0부터 255의 범위 안에 있으므로, 255로 값을 나눠줌으로써, 모든 값을 0부터 1 사이의 값으로 정규화합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제값을 one hot encoding으로 변경하기\n",
    "손실 함수에서 크로스 엔트로피를 계산하기 위해, 실제값은 one hot encoding 값으로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 다층퍼셉트론 그래프 구현하기\n",
    "텐서플로우로 아래의 다층퍼셉트론을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드랍 아웃 (Drop Out)\n",
    "마지막 히든레이어(h2)에 드랍 아웃을 설정하도록 하겠습니다.\n",
    "드랍 아웃이란 드랍 아웃이 설정된 레이어에서 랜덤으로 몇개의 노드를 비활성화하는 테크닉입니다.  \n",
    "미니 배치별로 사용된 노드들이 달라지기 때문에 결과적으로 모델의 분산을 줄여서 과대적합(overfitting)의 위험을 감소시킵니다.  \n",
    "아래 코드에서 keep_prob는 드랍 아웃 설정 변수로, 얼마만큼의 확률로 노드를 활성화할 것인지 결정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 하든 레이어2(h2)에 드랍아웃을 적용합니다.\n",
    "keep_prob의 값은 모델을 학습 또는 테스트할 때 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # hidden layer1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # hidden layer2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "    # output layer\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits= tf.matmul(h2_drop, w3) + b3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 조기 종료 (Early Stopping)\n",
    "학습 정확도가 계속해서 증가하는데, 검증 정확도가 꾸준히 떨어진다면,  \n",
    "학습을 조기 종료하여 과적합을 피할 수 있습니다.  \n",
    "조기 종료 이후, 지금까지 학습된 모델 중에서 최고로 높은 검증 정확도를 지닌 모델을  \n",
    "최종 모델로 선택할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 주기(Epoch)마다 검증 데이터로 검증 정확도를 측정합니다.  \n",
    "검증 정확도가 5번 연속으로 최고 검증 정확도보다 높지 않을 때 조기 종료를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 300\n",
    "batch_size = 1000\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "earlystop_threshold = 5\n",
    "earlystop_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 모델에 입력시킬 때(feed), 드랍아웃이 있을 경우, 항상 keep_prob를 설정해주셔야합니다.  \n",
    "학습 시, 10%의 드랍아웃을 하기 위해, keep_prob를 0.9로 설정합니다.  \n",
    "테스트 시, 드랍 아웃을 사용하지 않을 것이므로, keep_prob를 1.0으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train acc: 0.2932, val acc: 0.3102\n",
      "epoch: 1, train acc: 0.67224, val acc: 0.6858\n",
      "epoch: 2, train acc: 0.65674, val acc: 0.673\n",
      "epoch: 3, train acc: 0.67866, val acc: 0.7006\n",
      "epoch: 4, train acc: 0.70524, val acc: 0.7294\n",
      "epoch: 5, train acc: 0.73262, val acc: 0.752\n",
      "epoch: 6, train acc: 0.75858, val acc: 0.7746\n",
      "epoch: 7, train acc: 0.77988, val acc: 0.7923\n",
      "epoch: 8, train acc: 0.79884, val acc: 0.8124\n",
      "epoch: 9, train acc: 0.81392, val acc: 0.8272\n",
      "epoch: 10, train acc: 0.82718, val acc: 0.8387\n",
      "epoch: 11, train acc: 0.83678, val acc: 0.8467\n",
      "epoch: 12, train acc: 0.84676, val acc: 0.8562\n",
      "epoch: 13, train acc: 0.85592, val acc: 0.8631\n",
      "epoch: 14, train acc: 0.86306, val acc: 0.8688\n",
      "epoch: 15, train acc: 0.87038, val acc: 0.8757\n",
      "epoch: 16, train acc: 0.8765, val acc: 0.8811\n",
      "epoch: 17, train acc: 0.88264, val acc: 0.886\n",
      "epoch: 18, train acc: 0.8869, val acc: 0.8892\n",
      "epoch: 19, train acc: 0.8916, val acc: 0.8922\n",
      "epoch: 20, train acc: 0.89626, val acc: 0.8945\n",
      "epoch: 21, train acc: 0.9002, val acc: 0.8971\n",
      "epoch: 22, train acc: 0.9025, val acc: 0.8982\n",
      "epoch: 23, train acc: 0.90484, val acc: 0.9005\n",
      "epoch: 24, train acc: 0.90716, val acc: 0.9008\n",
      "epoch: 25, train acc: 0.911, val acc: 0.9039\n",
      "epoch: 26, train acc: 0.91418, val acc: 0.9074\n",
      "epoch: 27, train acc: 0.91656, val acc: 0.9088\n",
      "epoch: 28, train acc: 0.91836, val acc: 0.9098\n",
      "epoch: 29, train acc: 0.92062, val acc: 0.9118\n",
      "epoch: 30, train acc: 0.9224, val acc: 0.914\n",
      "epoch: 31, train acc: 0.92528, val acc: 0.9165\n",
      "epoch: 32, train acc: 0.92648, val acc: 0.9177\n",
      "epoch: 33, train acc: 0.92852, val acc: 0.9195\n",
      "epoch: 34, train acc: 0.93012, val acc: 0.9195\n",
      "epoch: 35, train acc: 0.93202, val acc: 0.9224\n",
      "epoch: 36, train acc: 0.9333, val acc: 0.9239\n",
      "epoch: 37, train acc: 0.93548, val acc: 0.9249\n",
      "epoch: 38, train acc: 0.93704, val acc: 0.9263\n",
      "epoch: 39, train acc: 0.93866, val acc: 0.9261\n",
      "overfitting warning: 0\n",
      "epoch: 40, train acc: 0.93944, val acc: 0.9275\n",
      "epoch: 41, train acc: 0.94094, val acc: 0.9284\n",
      "epoch: 42, train acc: 0.94162, val acc: 0.9291\n",
      "epoch: 43, train acc: 0.9428, val acc: 0.9299\n",
      "epoch: 44, train acc: 0.94446, val acc: 0.9312\n",
      "epoch: 45, train acc: 0.9452, val acc: 0.9329\n",
      "epoch: 46, train acc: 0.94694, val acc: 0.933\n",
      "epoch: 47, train acc: 0.94726, val acc: 0.9329\n",
      "overfitting warning: 0\n",
      "epoch: 48, train acc: 0.94814, val acc: 0.9333\n",
      "epoch: 49, train acc: 0.94944, val acc: 0.9347\n",
      "epoch: 50, train acc: 0.95032, val acc: 0.9348\n",
      "epoch: 51, train acc: 0.95198, val acc: 0.9361\n",
      "epoch: 52, train acc: 0.95188, val acc: 0.935\n",
      "epoch: 53, train acc: 0.95334, val acc: 0.9355\n",
      "overfitting warning: 0\n",
      "epoch: 54, train acc: 0.95398, val acc: 0.9361\n",
      "epoch: 55, train acc: 0.95514, val acc: 0.9375\n",
      "epoch: 56, train acc: 0.95674, val acc: 0.9379\n",
      "epoch: 57, train acc: 0.95632, val acc: 0.937\n",
      "epoch: 58, train acc: 0.95682, val acc: 0.9374\n",
      "overfitting warning: 0\n",
      "epoch: 59, train acc: 0.95864, val acc: 0.9392\n",
      "epoch: 60, train acc: 0.95892, val acc: 0.9389\n",
      "overfitting warning: 0\n",
      "epoch: 61, train acc: 0.9597, val acc: 0.9388\n",
      "overfitting warning: 1\n",
      "epoch: 62, train acc: 0.9609, val acc: 0.9381\n",
      "overfitting warning: 2\n",
      "epoch: 63, train acc: 0.96136, val acc: 0.9382\n",
      "overfitting warning: 3\n",
      "epoch: 64, train acc: 0.9615, val acc: 0.939\n",
      "overfitting warning: 4\n",
      "epoch: 65, train acc: 0.96302, val acc: 0.9399\n",
      "epoch: 66, train acc: 0.96342, val acc: 0.9395\n",
      "overfitting warning: 0\n",
      "epoch: 67, train acc: 0.96354, val acc: 0.9399\n",
      "epoch: 68, train acc: 0.96402, val acc: 0.94\n",
      "epoch: 69, train acc: 0.9644, val acc: 0.9397\n",
      "overfitting warning: 0\n",
      "epoch: 70, train acc: 0.9661, val acc: 0.9408\n",
      "epoch: 71, train acc: 0.96632, val acc: 0.9413\n",
      "epoch: 72, train acc: 0.96694, val acc: 0.942\n",
      "epoch: 73, train acc: 0.96798, val acc: 0.9418\n",
      "overfitting warning: 0\n",
      "epoch: 74, train acc: 0.96702, val acc: 0.9415\n",
      "epoch: 75, train acc: 0.96896, val acc: 0.9428\n",
      "epoch: 76, train acc: 0.96908, val acc: 0.9434\n",
      "epoch: 77, train acc: 0.9695, val acc: 0.944\n",
      "epoch: 78, train acc: 0.97016, val acc: 0.9435\n",
      "overfitting warning: 0\n",
      "epoch: 79, train acc: 0.97082, val acc: 0.9425\n",
      "overfitting warning: 1\n",
      "epoch: 80, train acc: 0.97146, val acc: 0.9444\n",
      "epoch: 81, train acc: 0.97262, val acc: 0.9449\n",
      "epoch: 82, train acc: 0.97174, val acc: 0.9448\n",
      "epoch: 83, train acc: 0.97146, val acc: 0.9448\n",
      "epoch: 84, train acc: 0.97342, val acc: 0.9463\n",
      "epoch: 85, train acc: 0.973, val acc: 0.9463\n",
      "epoch: 86, train acc: 0.97202, val acc: 0.9446\n",
      "epoch: 87, train acc: 0.97264, val acc: 0.9448\n",
      "overfitting warning: 0\n",
      "epoch: 88, train acc: 0.97352, val acc: 0.9456\n",
      "overfitting warning: 1\n",
      "epoch: 89, train acc: 0.97462, val acc: 0.9462\n",
      "overfitting warning: 2\n",
      "epoch: 90, train acc: 0.97454, val acc: 0.9469\n",
      "epoch: 91, train acc: 0.9743, val acc: 0.947\n",
      "epoch: 92, train acc: 0.97528, val acc: 0.9465\n",
      "overfitting warning: 0\n",
      "epoch: 93, train acc: 0.97514, val acc: 0.9459\n",
      "epoch: 94, train acc: 0.97384, val acc: 0.9455\n",
      "epoch: 95, train acc: 0.97448, val acc: 0.946\n",
      "overfitting warning: 0\n",
      "epoch: 96, train acc: 0.97586, val acc: 0.9466\n",
      "overfitting warning: 1\n",
      "epoch: 97, train acc: 0.97654, val acc: 0.9474\n",
      "epoch: 98, train acc: 0.97708, val acc: 0.947\n",
      "overfitting warning: 0\n",
      "epoch: 99, train acc: 0.97676, val acc: 0.9477\n",
      "epoch: 100, train acc: 0.97784, val acc: 0.9484\n",
      "epoch: 101, train acc: 0.97812, val acc: 0.9483\n",
      "overfitting warning: 0\n",
      "epoch: 102, train acc: 0.9777, val acc: 0.9463\n",
      "epoch: 103, train acc: 0.97808, val acc: 0.9477\n",
      "overfitting warning: 0\n",
      "epoch: 104, train acc: 0.97828, val acc: 0.9477\n",
      "overfitting warning: 1\n",
      "epoch: 105, train acc: 0.97866, val acc: 0.948\n",
      "overfitting warning: 2\n",
      "epoch: 106, train acc: 0.97858, val acc: 0.9497\n",
      "epoch: 107, train acc: 0.97842, val acc: 0.9493\n",
      "epoch: 108, train acc: 0.9796, val acc: 0.9489\n",
      "overfitting warning: 0\n",
      "epoch: 109, train acc: 0.97994, val acc: 0.9502\n",
      "epoch: 110, train acc: 0.97942, val acc: 0.9476\n",
      "epoch: 111, train acc: 0.97902, val acc: 0.9492\n",
      "epoch: 112, train acc: 0.98002, val acc: 0.9502\n",
      "epoch: 113, train acc: 0.98058, val acc: 0.9495\n",
      "overfitting warning: 0\n",
      "epoch: 114, train acc: 0.97974, val acc: 0.9483\n",
      "epoch: 115, train acc: 0.9793, val acc: 0.9485\n",
      "epoch: 116, train acc: 0.98054, val acc: 0.9499\n",
      "overfitting warning: 0\n",
      "epoch: 117, train acc: 0.98238, val acc: 0.9505\n",
      "epoch: 118, train acc: 0.98276, val acc: 0.9512\n",
      "epoch: 119, train acc: 0.98182, val acc: 0.9499\n",
      "epoch: 120, train acc: 0.98158, val acc: 0.9504\n",
      "epoch: 121, train acc: 0.9817, val acc: 0.9499\n",
      "overfitting warning: 0\n",
      "epoch: 122, train acc: 0.98142, val acc: 0.9505\n",
      "epoch: 123, train acc: 0.98262, val acc: 0.9502\n",
      "overfitting warning: 0\n",
      "epoch: 124, train acc: 0.98336, val acc: 0.9528\n",
      "epoch: 125, train acc: 0.9816, val acc: 0.9494\n",
      "epoch: 126, train acc: 0.98188, val acc: 0.9517\n",
      "overfitting warning: 0\n",
      "epoch: 127, train acc: 0.98324, val acc: 0.9528\n",
      "epoch: 128, train acc: 0.9837, val acc: 0.9515\n",
      "overfitting warning: 0\n",
      "epoch: 129, train acc: 0.9832, val acc: 0.9517\n",
      "epoch: 130, train acc: 0.98344, val acc: 0.9521\n",
      "overfitting warning: 0\n",
      "epoch: 131, train acc: 0.98364, val acc: 0.9536\n",
      "epoch: 132, train acc: 0.98394, val acc: 0.9546\n",
      "epoch: 133, train acc: 0.98348, val acc: 0.9542\n",
      "epoch: 134, train acc: 0.98394, val acc: 0.9536\n",
      "overfitting warning: 0\n",
      "epoch: 135, train acc: 0.98298, val acc: 0.9536\n",
      "epoch: 136, train acc: 0.98172, val acc: 0.9501\n",
      "epoch: 137, train acc: 0.98098, val acc: 0.9524\n",
      "epoch: 138, train acc: 0.98228, val acc: 0.9503\n",
      "overfitting warning: 0\n",
      "epoch: 139, train acc: 0.98166, val acc: 0.9521\n",
      "epoch: 140, train acc: 0.98176, val acc: 0.952\n",
      "overfitting warning: 0\n",
      "epoch: 141, train acc: 0.98402, val acc: 0.9545\n",
      "overfitting warning: 1\n",
      "epoch: 142, train acc: 0.98584, val acc: 0.9559\n",
      "epoch: 143, train acc: 0.987, val acc: 0.9573\n",
      "epoch: 144, train acc: 0.98236, val acc: 0.9556\n",
      "epoch: 145, train acc: 0.98648, val acc: 0.9571\n",
      "overfitting warning: 0\n",
      "epoch: 146, train acc: 0.98504, val acc: 0.9546\n",
      "epoch: 147, train acc: 0.98808, val acc: 0.9574\n",
      "epoch: 148, train acc: 0.98778, val acc: 0.9553\n",
      "epoch: 149, train acc: 0.98604, val acc: 0.9534\n",
      "epoch: 150, train acc: 0.9875, val acc: 0.9552\n",
      "overfitting warning: 0\n",
      "epoch: 151, train acc: 0.98786, val acc: 0.9573\n",
      "overfitting warning: 1\n",
      "epoch: 152, train acc: 0.98876, val acc: 0.9563\n",
      "overfitting warning: 2\n",
      "epoch: 153, train acc: 0.98938, val acc: 0.9581\n",
      "epoch: 154, train acc: 0.98974, val acc: 0.9565\n",
      "overfitting warning: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 155, train acc: 0.98864, val acc: 0.9563\n",
      "epoch: 156, train acc: 0.98904, val acc: 0.9556\n",
      "overfitting warning: 0\n",
      "epoch: 157, train acc: 0.9912, val acc: 0.9578\n",
      "overfitting warning: 1\n",
      "epoch: 158, train acc: 0.9911, val acc: 0.9566\n",
      "overfitting warning: 2\n",
      "epoch: 159, train acc: 0.9904, val acc: 0.9553\n",
      "overfitting warning: 3\n",
      "epoch: 160, train acc: 0.99088, val acc: 0.9563\n",
      "overfitting warning: 4\n",
      "epoch: 161, train acc: 0.991, val acc: 0.9575\n",
      "early stopped on 161\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], \n",
    "                               feed_dict={x: x_train[start: end], y: y_train[start: end], \n",
    "                                          keep_prob: 0.9})\n",
    "            start += batch_size; end += batch_size\n",
    "            # Compute train average loss\n",
    "            avg_loss += loss / iteration\n",
    "            \n",
    "        # Validate model\n",
    "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # train accuracy\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train,keep_prob: 1.0})\n",
    "        # validation accuarcy\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # validation loss\n",
    "        cur_val_loss = loss_op.eval({x: x_val, y: y_val,keep_prob: 1.0})\n",
    "        \n",
    "        print(\"epoch: \"+str(epoch)+\n",
    "              \", train acc: \" + str(cur_train_acc) +\n",
    "              \", val acc: \" + str(cur_val_acc) )\n",
    "              #', train loss: '+str(avg_loss)+\n",
    "              #', val loss: '+str(cur_val_loss))\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print(\"early stopped on \"+str(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"overfitting warning: \"+str(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else:\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            # Save the variables to file.\n",
    "            save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "        prev_train_acc = cur_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트\n",
    "검증 정확도가 가장 높은 모델을 대상으로 테스트를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "[Test Accuracy] : 0.9527\n"
     ]
    }
   ],
   "source": [
    "# Start testing\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"model/model.ckpt\")\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
