{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트맥스 (다중 분류 로지스틱 회귀 모델)\n",
    "M개의 입력을 받아 N개의 클래스로 출력하는 로지스틱 회귀 모델을 케라스로 구현해보도록 하겠습니다.  \n",
    "보통 다중 분류 로지스틱 회귀 모델을 소프트맥스(softmax)라고 부릅니다.  \n",
    "케라스에서 제공하는 MNIST 데이터를 사용하여 숫자를 0에서부터 9까지 분류해보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 손글씨 데이터를 다운로드 받아서 변수에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손글씨 데이터(X_train, X_test)가 가로 28픽셀, 세로 28픽셀로 구성된 것을 확인할 수 있습니다.  \n",
    "학습에 사용될 X_train은 총 60000개의 데이터, 테스트에 사용될 X_test는 총 10000개의 데이터가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data (count, row, column) : (60000, 28, 28)\n",
      "test data  (count, row, column) : (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data (count, row, column) : \" + str(X_train.shape) )\n",
    "print(\"test data  (count, row, column) : \" + str(X_test.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터의 하나를 샘플로 보도록 하겠습니다. 아래 보시는 것처럼, 각각의 픽셀은 0부터 255까지의 값을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습 시작에 앞서, 데이터를 정규화합니다.  \n",
    "정규화는 입력값을 0부터 1의 값으로 변경하게 됩니다.  \n",
    "정규화된 입력값은 경사하강법으로 모델 학습 시, 보다 쉽고 빠르게 최적의 W,B를 찾는 데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 명령어를 통해 정규화된 데이터를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333336\n",
      "  0.6862745  0.10196079 0.6509804  1.         0.96862745 0.49803922\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      "  0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215687 0.93333334 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.9843137\n",
      "  0.3647059  0.32156864 0.32156864 0.21960784 0.15294118 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07058824 0.85882354 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.7764706  0.7137255  0.96862745 0.94509804\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      "  0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05490196 0.00392157 0.6039216\n",
      "  0.99215686 0.3529412  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.54509807\n",
      "  0.99215686 0.74509805 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313726\n",
      "  0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13725491 0.94509804 0.88235295 0.627451   0.42352942 0.00392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31764707 0.9411765  0.99215686 0.99215686 0.46666667\n",
      "  0.09803922 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      "  0.5882353  0.10588235 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0627451  0.3647059  0.9882353\n",
      "  0.99215686 0.73333335 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.9764706\n",
      "  0.99215686 0.9764706  0.2509804  0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      "  0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15294118 0.5803922  0.8980392  0.99215686 0.99215686 0.99215686\n",
      "  0.98039216 0.7137255  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09411765 0.44705883\n",
      "  0.8666667  0.99215686 0.99215686 0.99215686 0.99215686 0.7882353\n",
      "  0.30588236 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.07058824 0.67058825 0.85882354 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.7647059  0.3137255  0.03529412 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21568628 0.6745098\n",
      "  0.8862745  0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "  0.52156866 0.04313726 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.53333336 0.99215686\n",
      "  0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train, y_test는 손글씨 데이터 (28*28 픽셀 데이터)에 해당하는 숫자를 나타냅니다.  \n",
    "y_train은 총 6만개, y_test는 총 1만개의 숫자를 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train target (count) : (60000,)\n",
      "test target  (count) : (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train target (count) : \" + str(y_train.shape) )\n",
    "print(\"test target  (count) : \" + str(y_test.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여, y_train과 y_test에서 샘플로 숫자를 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from train : 5\n",
      "sample from test : 7\n"
     ]
    }
   ],
   "source": [
    "print(\"sample from train : \" + str(y_train[0]) )\n",
    "print(\"sample from test : \" + str(y_test[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습에서는 28*28 픽셀의 지역적인 정보를 사용하지 않고, 단순히 정규화된 입력값만을 가지고,  \n",
    "숫자 분류를 할 것이기 때문에, 행과 열의 구분 없이, 단순히 784 길이의 배열로 데이터를 단순화시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784 #28*28 \n",
    "X_train = X_train.reshape(60000, input_dim) \n",
    "X_test = X_test.reshape(10000, input_dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 명령어를 실행하여, 현재 우리의 데이터가 2차원 데이터가 아닌 단순한 1차원 데이터로 변경된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스가 어떻게 10개의 숫자를 구분하는 지가 이번 실습의 핵심입니다.  \n",
    "소프트맥스는 정규화된 여러개의 로지스틱회귀로 구성되어 있으며,  \n",
    "10개의 로지스틱회귀를 배열로 나타낼 경우, [L0, L1, L2, L3, L4, L5, L6, L7, L8, L9]으로 나타낼 수 있습니다.  \n",
    "각 인덱스는 각각의 숫자를 의미합니다.  \n",
    "로지스틱회귀이기 때문에, 각 L의 값은 0부터 1의 값을 가지고 있으며, 만약 출력값이 [0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0]일 경우,  \n",
    "가장 높은 확률을 가진 첫번째 인덱스, 즉 0이 소프트맥스의 출력값이라고 볼 수 있습니다.\n",
    "\n",
    "학습 시, y값과의 cross entropy를 측정해야하므로, 아래의 코드를 실행하여 y를 one hot encoding으로 변환시켜줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행하여, 5였던 값이, one hot encoding으로 변환되어,  \n",
    "클래스 갯수만큼의 길이를 갖는 벡터로 변경이 되었고, 5에 해당되는 인덱스의 값만 1인 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스의 Sequential()을 사용하여 간단하게 소프트맥스를 구현할 수 있습니다.  \n",
    "총 784개(28*28)의 입력을 받아서, 10개의 시그모이드 값을 출력하는 모델을 아래의 코드를 실행하여 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(input_dim=input_dim, units = 10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 학습을 진행합니다.  \n",
    "10개의 클래스로 분류할 것이기 때문에, categorical_crossentropy를 비용함수로 사용한 경사하강법으로 최적의 W와 biases를 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 2.2915 - acc: 0.1608\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.0073 - acc: 0.3549\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7861 - acc: 0.5324\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.6093 - acc: 0.6270\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.4670 - acc: 0.6843\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.3514 - acc: 0.7229A: 0s - loss: 1.3897 - acc: \n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.2566 - acc: 0.7495\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.1779 - acc: 0.7679\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.1119 - acc: 0.7822\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.0558 - acc: 0.7930\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.0077 - acc: 0.8017\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.9659 - acc: 0.8085\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.9293 - acc: 0.8138\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.8971 - acc: 0.8187\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.8685 - acc: 0.8223\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.8429 - acc: 0.8259\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.8198 - acc: 0.8287\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.7989 - acc: 0.8314\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.7799 - acc: 0.8341\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.7626 - acc: 0.8364\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.7466 - acc: 0.8381\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.7319 - acc: 0.8398\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.7183 - acc: 0.8420\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.7057 - acc: 0.8438\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6940 - acc: 0.8452\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6831 - acc: 0.8466\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6728 - acc: 0.8483\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6632 - acc: 0.8498\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6541 - acc: 0.8509\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6456 - acc: 0.8520\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6376 - acc: 0.8530\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6300 - acc: 0.8539\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6228 - acc: 0.8547\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6159 - acc: 0.8556\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.6094 - acc: 0.8567\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.6032 - acc: 0.8573\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5973 - acc: 0.8583\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5917 - acc: 0.8591\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5863 - acc: 0.8598\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5811 - acc: 0.8605\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5761 - acc: 0.8613\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5714 - acc: 0.8621\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5668 - acc: 0.8629\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5624 - acc: 0.8636\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5582 - acc: 0.8643\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5542 - acc: 0.8649\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5503 - acc: 0.8656\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.5465 - acc: 0.8662\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5428 - acc: 0.8665\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5393 - acc: 0.8671\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.5359 - acc: 0.8678\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5326 - acc: 0.8683\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5294 - acc: 0.8691\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5263 - acc: 0.8696\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.5233 - acc: 0.8699\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5204 - acc: 0.8702\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5176 - acc: 0.8709\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5149 - acc: 0.8712\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5122 - acc: 0.8718\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5097 - acc: 0.8723\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5071 - acc: 0.8727\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.5047 - acc: 0.8732\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.5023 - acc: 0.8735\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.5000 - acc: 0.8737\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4977 - acc: 0.8741\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4956 - acc: 0.8745\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4934 - acc: 0.8748\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4913 - acc: 0.8752\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4893 - acc: 0.8755\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4873 - acc: 0.8759\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4853 - acc: 0.8762\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4834 - acc: 0.8766\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4816 - acc: 0.8770\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4798 - acc: 0.8774\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4780 - acc: 0.8778\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4762 - acc: 0.8780\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.4745 - acc: 0.8783\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4729 - acc: 0.8785\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.4712 - acc: 0.8790\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.4696 - acc: 0.8794\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4681 - acc: 0.8796\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.4666 - acc: 0.8799\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.4651 - acc: 0.8802\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.4636 - acc: 0.8804\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4621 - acc: 0.8807\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4607 - acc: 0.8810\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4593 - acc: 0.8812\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.4580 - acc: 0.8814\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4566 - acc: 0.8818\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.4553 - acc: 0.8821\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.4540 - acc: 0.8823\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4528 - acc: 0.8825\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4515 - acc: 0.8827\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4503 - acc: 0.8828\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4491 - acc: 0.8831\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4479 - acc: 0.8832\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4467 - acc: 0.8833\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4456 - acc: 0.8836\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4445 - acc: 0.8837\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.4434 - acc: 0.8839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1813b71c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "model.fit(X_train, y_train, batch_size=2048, epochs=100, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 진행하여, 정확도를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/step\n",
      "Test accuracy: 0.8912\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여, 소프트맥스 모델의 구조를 쉽게 시각화 할 수 있습니다.  \n",
    "총 10개의 로지스틱회귀가 있고, 각 로지스틱회귀는 784개의 weight와 1개의 bias를 갖고 있기 때문에,  \n",
    "총 7850 (785*10)개의 Param이 있는 것을 보실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 레이어에 존재하는 w1, w2,...,w784, b1, b2,..., b10은 아래의 명령어로 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(784, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
