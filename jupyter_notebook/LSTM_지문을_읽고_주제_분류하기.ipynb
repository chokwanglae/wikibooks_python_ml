{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM으로 지문을 읽어서 주제를 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM을 사용하여 각 지문의 주제를 분류하는 예제를 다뤄보겠습니다.  \n",
    "아래 실습을 위해 입력값을 팬더스 데이터프레임에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_dict_list = [\n",
    "         {'paragraph': 'dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "         {'paragraph': 'service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "         {'paragraph': 'portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "         {'paragraph': 'we started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "         {'paragraph': 'the biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "         {'paragraph': 'the garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "         {'paragraph': 'our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "         {'paragraph': 'what i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "         {'paragraph': 'the drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "         {'paragraph': 'despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "    \n",
    "         {'paragraph': 'the four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "         {'paragraph': 'the briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "         {'paragraph': 'stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "         {'paragraph': 'when it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "         {'paragraph': 'the team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "         {'paragraph': 'the perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "         {'paragraph': 'liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "         {'paragraph': 'alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "         {'paragraph': 'but the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "         {'paragraph': 'then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the third set there were exhilarating rallies with both chasing to the net both retrieving what looked like winning shots nadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]\n",
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df = df[['paragraph', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주제가 **음식**인 지문을 몇개 출력해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph category\n",
       "0  dishplace is located in sunnyvale downtown the...     food\n",
       "1  service can be slower during busy hours but ou...     food\n",
       "2  portions are huge both french toast and their ...     food\n",
       "3  we started with apps going the chicken and waf...     food\n",
       "4  the biscuits and gravy was too salty two peopl...     food"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주제가 **스포츠**엔 지문을 몇개 출력해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category\n",
       "15  the perseyside outfit finished in fourth place...   sports\n",
       "16  liverpool fc will return to premier league act...   sports\n",
       "17  alisson signed for liverpool fc from as roma t...   sports\n",
       "18  but the rankings during that run-in to new yor...   sports\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "모든 딥러닝 모델들이 수학에 근간하듯이, LSTM 역시 수학을 기반으로한 모델이므로, 입력값을 수치값으로 변경해주어야 합니다. 텍스트인 입력값을 수치로 변경하기 위해, 지문에 사용된 모든 단어들을 모아 중복을 제거하여 단어 리스트를 만듭니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = set()\n",
    "df['paragraph'].str.lower().str.split().apply(results.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리스트에 들어간 순서(인덱스)가 곧 단어를 표현하는 숫자가 됩니다. word2idx 딕셔너리를 생성하여 추후 쉽게 단어를 숫자로 전환할 수 있도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word= dict(enumerate(results))\n",
    "word2idx = {v: k for k, v in idx2word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 만약 문장이 \"one week\"일 경우, 이번 실습을 통해 구현될 모델의 입력값은 [15, 4]가 됩니다.  \n",
    "아래 코드를 실행하여 \"bread\"의 인덱스를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"bread\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행하여 \"tournament\"의 인덱스를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"tournament\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2idx를 활용하여 모든 지문을 수치로 전환하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_paragraph(paragraph):\n",
    "    words = paragraph.split(\" \")\n",
    "    encoded = []\n",
    "    for word in words:\n",
    "        encoded.append([word2idx[word]])\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "df[\"enc_paragraph\"] = df.paragraph.apply(encode_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 항목(food, sports) 역시 수치로 변경해야합니다. 분류 항목은 one hot encoding으로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_category(category):\n",
    "    if category == \"food\":\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]\n",
    "    \n",
    "df[\"enc_category\"] = df.category.apply(encode_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습에 사용할 RNN은 Dynamic RNN입니다. Dynamic RNN은 입력값의 다양한 길이를 고려하여 결과값을 출력합니다.  \n",
    "Dynamic RNN이 각 입력값의 실제 길이(단어수)를 알 수 있도록 각 지문별 단어수를 미리 계산해놓도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cnt(paragraph):\n",
    "    return len(paragraph.split(\" \"))\n",
    "\n",
    "df[\"seq_length\"] = df.paragraph.apply(word_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우의 RNN은 항상 같은 길이의 입력 시퀀스를 받아야하므로, 길이가 작은 입력 시퀀스는 패딩을 추가적으로 넣어서 모든 시퀀스의 길이를 동일하게 설정합니다. 패딩이 RNN 계산에 영향을 끼치지 않도록, Dynamic RNN은 패딩 이전의 입력 시퀀스의 실제길이를 파라미터로 받아 계산에서 패딩을 제외시킵니다.  \n",
    "\n",
    "이번 실습에서는 최고로 긴 지문의 단어수를 구한 후, 모든 지문에 패딩을 집어넣어 최고로 긴 지문과 동일한 길이를 갖도록 하겠습니다.  \n",
    "아래의 코드를 실행하여 최고로 긴 지문의 단어수를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "max_word_cnt = 0\n",
    "for row in df['paragraph']:\n",
    "    if len(row.split(\" \")) > max_word_cnt:\n",
    "        max_word_cnt = len(row.split(\" \"))\n",
    "\n",
    "print (max_word_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 지문에 패딩을 집어넣어 최고로 긴 지문과 동일한 길이를 갖도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(enc_paragraph):\n",
    "    seq_length = len(enc_paragraph)\n",
    "    for i in range(seq_length,max_word_cnt):\n",
    "        enc_paragraph.append([-1])\n",
    "    \n",
    "    return enc_paragraph\n",
    "\n",
    "df[\"enc_paragraph\"] = df.enc_paragraph.apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여, 패딩이 들어간 문장을 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512], [228], [486], [385], [367], [346], [96], [228], [320], [40], [299], [88], [164], [472], [125], [350], [356], [476], [370], [181], [359], [160], [389], [306], [485], [363], [318], [221], [476], [382], [114], [435], [39], [198], [383], [219], [22], [6], [447], [174], [187], [318], [502], [530], [355], [9], [534], [349], [504], [299], [12], [476], [336], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"enc_paragraph\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 수치화된 모든 데이터를 확인해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "      <th>enc_category</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[512], [228], [486], [385], [367], [346], [96...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[459], [125], [350], [250], [181], [174], [38...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[327], [522], [69], [352], [152], [388], [363...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[54], [266], [102], [380], [194], [299], [529...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[299], [4], [363], [460], [439], [44], [270],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[299], [202], [381], [6], [383], [222], [461]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[224], [66], [439], [215], [318], [124], [299...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[524], [318], [231], [419], [499], [386], [8]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[299], [153], [221], [468], [122], [311], [38...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[362], [299], [375], [187], [193], [149], [29...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[299], [280], [216], [295], [329], [87], [264...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[299], [212], [439], [410], [205], [108], [31...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[307], [121], [236], [354], [420], [232], [32...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[294], [472], [221], [476], [75], [399], [385...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[299], [314], [245], [141], [440], [132], [23...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[299], [60], [172], [313], [385], [491], [114...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[282], [143], [170], [115], [476], [281], [36...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[395], [453], [435], [282], [143], [232], [33...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[164], [299], [30], [181], [245], [189], [476...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[408], [221], [299], [480], [527], [191], [40...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  \\\n",
       "0   dishplace is located in sunnyvale downtown the...     food   \n",
       "1   service can be slower during busy hours but ou...     food   \n",
       "2   portions are huge both french toast and their ...     food   \n",
       "3   we started with apps going the chicken and waf...     food   \n",
       "4   the biscuits and gravy was too salty two peopl...     food   \n",
       "5   the garlic fries were a great starter (and a h...     food   \n",
       "6   our meal was excellent i had the pasta ai form...     food   \n",
       "7   what i enjoy most about palo alto is so many r...     food   \n",
       "8   the drinks came out fairly quickly a good two ...     food   \n",
       "9   despite the not so good burger the service was...     food   \n",
       "10  the four reigning major champions simona halep...   sports   \n",
       "11  the briton was seeded nn7 here last year befor...   sports   \n",
       "12  stephens surged her way back from injury in st...   sports   \n",
       "13  when it came to england chances in the world c...   sports   \n",
       "14  the team that eliminated russia – croatia – al...   sports   \n",
       "15  the perseyside outfit finished in fourth place...   sports   \n",
       "16  liverpool fc will return to premier league act...   sports   \n",
       "17  alisson signed for liverpool fc from as roma t...   sports   \n",
       "18  but the rankings during that run-in to new yor...   sports   \n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports   \n",
       "\n",
       "                                        enc_paragraph enc_category  seq_length  \n",
       "0   [[512], [228], [486], [385], [367], [346], [96...       [1, 0]          53  \n",
       "1   [[459], [125], [350], [250], [181], [174], [38...       [1, 0]          19  \n",
       "2   [[327], [522], [69], [352], [152], [388], [363...       [1, 0]          42  \n",
       "3   [[54], [266], [102], [380], [194], [299], [529...       [1, 0]          43  \n",
       "4   [[299], [4], [363], [460], [439], [44], [270],...       [1, 0]          82  \n",
       "5   [[299], [202], [381], [6], [383], [222], [461]...       [1, 0]          24  \n",
       "6   [[224], [66], [439], [215], [318], [124], [299...       [1, 0]          50  \n",
       "7   [[524], [318], [231], [419], [499], [386], [8]...       [1, 0]          43  \n",
       "8   [[299], [153], [221], [468], [122], [311], [38...       [1, 0]          49  \n",
       "9   [[362], [299], [375], [187], [193], [149], [29...       [1, 0]          82  \n",
       "10  [[299], [280], [216], [295], [329], [87], [264...       [0, 1]          65  \n",
       "11  [[299], [212], [439], [410], [205], [108], [31...       [0, 1]          88  \n",
       "12  [[307], [121], [236], [354], [420], [232], [32...       [0, 1]          91  \n",
       "13  [[294], [472], [221], [476], [75], [399], [385...       [0, 1]          71  \n",
       "14  [[299], [314], [245], [141], [440], [132], [23...       [0, 1]          70  \n",
       "15  [[299], [60], [172], [313], [385], [491], [114...       [0, 1]          30  \n",
       "16  [[282], [143], [170], [115], [476], [281], [36...       [0, 1]          35  \n",
       "17  [[395], [453], [435], [282], [143], [232], [33...       [0, 1]          30  \n",
       "18  [[164], [299], [30], [181], [245], [189], [476...       [0, 1]          63  \n",
       "19  [[408], [221], [299], [480], [527], [191], [40...       [0, 1]          65  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습을 위해 아래의 값들이 사용됩니다.  \n",
    "enc_paragraph: input   \n",
    "enc_category: target  \n",
    "seq_length: second parameter of Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_paragraph = np.array(df.enc_paragraph.tolist())\n",
    "enc_category = np.array(df.enc_category.tolist())\n",
    "seq_length = np.array(df.seq_length.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = enc_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = enc_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력값의 형태를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 91, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 20개의 학습 데이터가 91개의 인덱스로 구성되어 있음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행하여, 총 20개의 실제값(target)이 2d 벡터 형태로 존재하는 것을 확인할 수 있습니다.  \n",
    "[1, 0] : food  \n",
    "[0, 1] : sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우로 지문을 읽고 주제를 분류하는 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 다이어그램 그대로 텐서플로우 모델을 구현해보도록 하겠습니다. 모델은 크게 두 단계로 나눠서 설명할 수 있습니다.  \n",
    "1. 문맥 벡터(contextualized vector) 생성 단계  \n",
    "   1) 단어를 인덱스로 변환합니다. (인덱스는 단어에 해당되는 숫자일뿐, 단어간의 유사도는 가지고 있지않습니다.)  \n",
    "   2) 인덱스를 임베딩으로 변환합니다. 임베딩은 학습 과정을 통해 단어 유사도를 포함하게되어 문맥벡터 생성에 도움을 줍니다.  \n",
    "   3) lstm에 임베딩된 시퀀스를 입력하여, 최종 상태값을 출력합니다. 이 최종 상태값이 문맥 벡터입니다.\n",
    "2. 주제(food, sportx) 분류 단계  \n",
    "   1) 문맥 벡터를 dense layer에 입력합니다.  \n",
    "   2) dense layer의 출력값을 노드가 2개인 dense layer에 입력합니다.  \n",
    "   3) 노드가 2개인 dense layer의 출력값을 소프트맥스에 입력시켜서, food, sports에 대한 예측값(prediction)을 구합니다.  \n",
    "\n",
    "예측값과 실제값(target)의 크로스 엔트로피를 줄여나가는 과정을 통해 모델은 학습됩니다. 실습은 Adam Optimizer를 사용하여 학습하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/lstm_model_overview.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/lstm_model_overview.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 300\n",
    "\n",
    "# define input\n",
    "X = tf.placeholder(tf.float32, [None,max_word_cnt,1])\n",
    "# define output\n",
    "y = tf.placeholder(tf.int32, [None, 2])\n",
    "\n",
    "# word embedding layer\n",
    "embedding = tf.layers.dense(X, 5)\n",
    "# LSTM cell\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\n",
    "# get output and state (we only use state in this example)\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding, dtype=tf.float32, sequence_length=seq_length)\n",
    "# state is contextual vector which is input to MLP classifier (dense layer) below\n",
    "dense_layer = tf.layers.dense(state.h, 32)\n",
    "# logit has two nodes for cross entropy with one hot encoding target\n",
    "logits = tf.layers.dense(dense_layer, 2)\n",
    "# loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# Adam Optimizor\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 요악\n",
    "모델에 대한 간략한 요약은 다음과 같습니다.  \n",
    "[문맥 벡터 생성]  \n",
    "입력값은 단어들의 인덱스이며, 그 길이는 항상 91입니다.  \n",
    "임베딩 레이어는 인덱스를 받아, 5차원 벡터의 임베딩을 출력합니다.  \n",
    "LSTM 셀은 64차원 벡터의 상태값을 출력합니다.  \n",
    "\n",
    "[문맥 벡터를 사용하여 지문의 주제 분류하기]  \n",
    "주제 분류는 두개의 dense layer를 사용합니다.  \n",
    "첫번째 dense layer는 32개의 노드를 가지고 있습니다.  \n",
    "두번째 dense layer는 2개의 노드를 가지고 있으며, 이 2개의 노드는 소프트맥스의 입력값으로 들어갑니다.  \n",
    "소프트맥스는 각 분류값에 해당할 확률을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 91, 1), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 91, 5), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 64) dtype=float32>)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(embedding)\n",
    "print(state)\n",
    "print(dense_layer)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.674189, accuracy: 0.55\n",
      "epoch: 50, loss: 0.436392, accuracy: 0.8\n",
      "epoch: 100, loss: 0.23460719, accuracy: 0.9\n",
      "epoch: 150, loss: 0.03331802, accuracy: 1.0\n",
      "epoch: 200, loss: 0.0071475618, accuracy: 1.0\n",
      "epoch: 250, loss: 0.0022903415, accuracy: 1.0\n",
      "epoch: 300, loss: 0.0011263994, accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        sess.run(optimizer, feed_dict={X: train_X, y: train_Y})\n",
    "        train_loss = sess.run(loss, feed_dict={X: train_X, y: train_Y})\n",
    "\n",
    "        if epoch == 1 or epoch % 50 == 0:\n",
    "            preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "            correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            cur_acc = accuracy.eval({X: train_X, y: train_Y})\n",
    "            print(\"epoch: \"+ str(epoch)+\", loss: \"+str(train_loss)+\", accuracy: \"+str(cur_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 지문들이 충분히 학습되어, 학습 정확도가 100%인 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "arXiv:1708.00107 [cs.CL]  [https://arxiv.org/abs/1708.00107]  \n",
    "Learned in Translation: Contextualized Word Vectors  \n",
    "Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
